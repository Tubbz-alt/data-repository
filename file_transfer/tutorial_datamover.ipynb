{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python datamover module tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datamover` Python module provides high-level functionalities to transfer files in between different file resources (local, GitHub, S3 buckets and FTP-drives). To activate the functionalities, import the library functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import datamover as dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module provide for each of the file resources a class, with for each of the classes the methods `download_file` and `list_files` defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function LocalConnector.list_files at 0x7ff720d4df28> <function LocalConnector.download_file at 0x7ff720d4dea0>\n",
      "<function S3Connector.list_files at 0x7ff720d4e510> <function S3Connector.download_file at 0x7ff720d4e378>\n",
      "<function FTPConnector.list_files at 0x7ff720d4e840> <function FTPConnector.list_files at 0x7ff720d4e840>\n",
      "<function GithubConnector.list_files at 0x7ff720d4e1e0> <function GithubConnector.download_file at 0x7ff720d4e158>\n"
     ]
    }
   ],
   "source": [
    "print(dm.LocalConnector.list_files, dm.LocalConnector.download_file)\n",
    "print(dm.S3Connector.list_files, dm.S3Connector.download_file)\n",
    "print(dm.FTPConnector.list_files, dm.FTPConnector.list_files)\n",
    "print(dm.GithubConnector.list_files, dm.GithubConnector.download_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the S3 bucket provides an essential part in the Enram data infrastructure, an additional class `S3EnramHandler` is available, providing the required functions to handle the enram bucket. As the class is inherited from the `S3Connector`, those functions are available as well in the S3 handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count_enram_coverage', 'create_zip_version', 'download_file', 'key_exists', 'list_files', 'upload_enram_file', 'upload_file']\n"
     ]
    }
   ],
   "source": [
    "# print available methods:\n",
    "print([method for method in dir(dm.S3EnramHandler)  if not method.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transfer files, `Transporter` classes are available to define specific transfers. Currently a `LocalToS3` and a `BaltradToS3` are defined to manage the file transfer from respectively a local file resource and the Baltrad file server to the Enram S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datamover.transporters.LocalToS3'> <class 'datamover.transporters.BaltradToS3'>\n"
     ]
    }
   ],
   "source": [
    "print(dm.LocalToS3, dm.BaltradToS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a enram file managament perspective, the `S3EnramHandler`, together with the transporter classes are most relevant, as explained in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the S3 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access rights to the S3 instance are managed indirectly, using the `~/.aws/credentials` file and by attributing the proper rights to the user in the AWS console. When the user rights are configured and the proper policy is attributed, connection to the S3 bucket from the `datamover` package is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s3 = dm.S3Connector(\"lw-enram\") # analog for S3EnramHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lw-enram'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for S3 file checks are provided to support file checks and enlisting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if a file(path) already exists on the S3 bucket:\n",
    "s3.key_exists('cz/brd/2017/04/09/23/czbrd_vp_20170409230000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the file listing provides a generator, different options are available to have an overview list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cz/brd/2016/09/23/00/czbrd_vp_20160923T0000Z_0x5.h5',\n",
       " 'cz/brd/2016/09/23/00/czbrd_vp_20160923T0015Z_0x5.h5',\n",
       " 'cz/brd/2016/09/23/00/czbrd_vp_20160923T0030Z_0x5.h5',\n",
       " 'cz/brd/2016/09/23/00/czbrd_vp_20160923T0045Z_0x5.h5'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(s3.list_files(path='cz/brd/2016/09/23/00')) # using set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cz/brd/2016/09/23/00/czbrd_vp_20160923T0000Z_0x5.h5\n",
      "cz/brd/2016/09/23/00/czbrd_vp_20160923T0015Z_0x5.h5\n",
      "cz/brd/2016/09/23/00/czbrd_vp_20160923T0030Z_0x5.h5\n",
      "cz/brd/2016/09/23/00/czbrd_vp_20160923T0045Z_0x5.h5\n"
     ]
    }
   ],
   "source": [
    "for filepath in s3.list_files(path='cz/brd/2016/09/23/00'):\n",
    "    print(filepath)\n",
    "    # do something..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `datamover` is just a thin layer around the [boto3](http://boto3.readthedocs.io/en/latest/index.html) package, the other boto3 S3 client options are still available to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abort_multipart_upload', 'can_paginate', 'complete_multipart_upload', 'copy', 'copy_object', 'create_bucket', 'create_multipart_upload', 'delete_bucket', 'delete_bucket_analytics_configuration', 'delete_bucket_cors', 'delete_bucket_encryption', 'delete_bucket_inventory_configuration', 'delete_bucket_lifecycle', 'delete_bucket_metrics_configuration', 'delete_bucket_policy', 'delete_bucket_replication', 'delete_bucket_tagging', 'delete_bucket_website', 'delete_object', 'delete_object_tagging', 'delete_objects', 'download_file', 'download_fileobj', 'exceptions', 'generate_presigned_post', 'generate_presigned_url', 'get_bucket_accelerate_configuration', 'get_bucket_acl', 'get_bucket_analytics_configuration', 'get_bucket_cors', 'get_bucket_encryption', 'get_bucket_inventory_configuration', 'get_bucket_lifecycle', 'get_bucket_lifecycle_configuration', 'get_bucket_location', 'get_bucket_logging', 'get_bucket_metrics_configuration', 'get_bucket_notification', 'get_bucket_notification_configuration', 'get_bucket_policy', 'get_bucket_replication', 'get_bucket_request_payment', 'get_bucket_tagging', 'get_bucket_versioning', 'get_bucket_website', 'get_object', 'get_object_acl', 'get_object_tagging', 'get_object_torrent', 'get_paginator', 'get_waiter', 'head_bucket', 'head_object', 'list_bucket_analytics_configurations', 'list_bucket_inventory_configurations', 'list_bucket_metrics_configurations', 'list_buckets', 'list_multipart_uploads', 'list_object_versions', 'list_objects', 'list_objects_v2', 'list_parts', 'meta', 'put_bucket_accelerate_configuration', 'put_bucket_acl', 'put_bucket_analytics_configuration', 'put_bucket_cors', 'put_bucket_encryption', 'put_bucket_inventory_configuration', 'put_bucket_lifecycle', 'put_bucket_lifecycle_configuration', 'put_bucket_logging', 'put_bucket_metrics_configuration', 'put_bucket_notification', 'put_bucket_notification_configuration', 'put_bucket_policy', 'put_bucket_replication', 'put_bucket_request_payment', 'put_bucket_tagging', 'put_bucket_versioning', 'put_bucket_website', 'put_object', 'put_object_acl', 'put_object_tagging', 'restore_object', 'upload_file', 'upload_fileobj', 'upload_part', 'upload_part_copy', 'waiter_names']\n"
     ]
    }
   ],
   "source": [
    "print([method for method in dir(s3.s3client)  if not method.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Baltrad FTP to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have access to the Baltrad-server, a credentials file (`creds.py`) is required, defining the variables URL, LOGIN and PASSWORD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from creds import URL, LOGIN, PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transporter class `BaltradToS3` supports the file transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "btos = dm.BaltradToS3(URL, LOGIN, PASSWORD, \"lw-enram\", profile_name=\"lw-enram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessity of the `profile_name` depends from your AWS account setup. If you're default profile has the appropriate policy rights (as it is with the EC2 instance running the daily cron job), the AWS package will automatically use the default credentials and you do not need to specify the profile to use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transfer of files is provided by the `transfer` method. It is possible to limit the scope of the file transfer by defining a name match string. As a user, you can decide to overwrite the S3 bucket files or not. Furthermore, for testing purposes, a `limit` option has been provided and the option to print the transfers to `stdout`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bewid_vp_20180113T1700Z_0x5.h5 is succesfully transferred to S3 bucket\n",
      "deboo_vp_20180112T2330Z_0x5.h5 is succesfully transferred to S3 bucket\n",
      "deboo_vp_20180112T2345Z_0x5.h5 is succesfully transferred to S3 bucket\n",
      "deboo_vp_20180113T0000Z_0x5.h5 is succesfully transferred to S3 bucket\n",
      "deboo_vp_20180113T0015Z_0x5.h5 is succesfully transferred to S3 bucket\n"
     ]
    }
   ],
   "source": [
    "# transfer files with _vp_ in the name, overwriting existing files and limiting the transferred files to 5:\n",
    "btos.transfer(name_match=\"_vp_\", overwrite=True, \n",
    "              limit=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the transfer are logged in the atributes `btos.transferred` and `btos.stalled`. A combined report can be written to a file `log_file_transfer`. The `transfertype` option provides the user the ability to have a custom message in the transfer header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bewid_vp_20180113T1700Z_0x5.h5',\n",
       " 'deboo_vp_20180112T2330Z_0x5.h5',\n",
       " 'deboo_vp_20180112T2345Z_0x5.h5',\n",
       " 'deboo_vp_20180113T0000Z_0x5.h5',\n",
       " 'deboo_vp_20180113T0015Z_0x5.h5']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btos.transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "btos.report(reset_file=True, transfertype=\"Baltrad to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log is written to a file `log_file_transfer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\r\n",
      "Data transfer at 2018-01-15 15:15 from Baltrad to S3:\r\n",
      "-------------------------------------------------------\r\n",
      "\r\n",
      "Files not transferred:\r\n",
      "\r\n",
      "\r\n",
      "Files succesfully transferred:\r\n",
      "bewid_vp_20180113T1700Z_0x5.h5\r\n",
      "deboo_vp_20180112T2330Z_0x5.h5\r\n",
      "deboo_vp_20180112T2345Z_0x5.h5\r\n",
      "deboo_vp_20180113T0000Z_0x5.h5\r\n",
      "deboo_vp_20180113T0015Z_0x5.h5\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat log_file_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transporter classes provide direct access to the individual connectors of the transfer, analog as the usage of the connector as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btos.s3.key_exists('de/boo/2018/01/13/00/deboo_vp_20180113T0015Z_0x5.h5') # S3 check for existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de/boo/2018/01/13/00/deboo_vp_20180113T0000Z_0x5.h5',\n",
       " 'de/boo/2018/01/13/00/deboo_vp_20180113T0015Z_0x5.h5',\n",
       " 'de/boo/2018/01/13/00/deboo_vp_20180113T0045Z_0x5.h5'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(btos.s3.list_files(path='de/boo/2018/01/13/00')) # S3 file listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deboo_vp_20180113T0015Z_0x5.h5'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(btos.ftp.list_files(name_match=\"deboo_vp_20180113T0015Z_0x5.h5\")) # ftp file listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local files to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transfer from a local file directory towards the S3 Bucket is similar in the API, with the `transfer` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ltos = dm.LocalToS3(filepath=\"../example_data/\", bucket_name=\"lw-enram\", \n",
    "                    profile_name=\"lw-enram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ltos.transfer(name_match=\"_vp_\", overwrite=False, \n",
    "              limit=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ltos.transferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 enram handler functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `S3EnramHandler` class provides some additional functions to support the enram infrastructure:\n",
    "* coverage check: check how many files are available for a specific time basis and get the most recent file for each of the country/radar combination\n",
    "* zip file creation for bulk data transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bird profile data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s3enram = dm.S3EnramHandler(\"lw-enram\", profile_name=\"lw-enram\") # Connecto to S3 client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data coverage for each radar can be derived for multiple temporal intervals: day | month | year. For the [heatmap on the repository](http://enram.github.io/data-repository/), the daily counts are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Rerun file list overview to extract the current coverage\n",
    "coverage_day, _ = s3enram.count_enram_coverage(level='day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark the usage of the `_` to ignore the second output of the function, which is the information on the most recent available file for each radar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, derive the number of files available for April 7th 2017 for the `tra` radar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_day['frtra 2017-04-07']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same aggregation function can be used for monthly and yearly counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coverage_year, _ = s3enram.count_enram_coverage(level='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, derive the yearly counts for the Belgian radars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bejab 2016': 5102,\n",
       " 'bewid 2016': 4905,\n",
       " 'bewid 2017': 33,\n",
       " 'bewid 2018': 2,\n",
       " 'bezav 2016': 5491}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in coverage_year.items() if k.startswith(\"be\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts can be exported to a CSV-file as well, available as general `datamover` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"my_coverage_filename.csv\", 'w') as outfile:\n",
    "    dm.coverage_to_csv(outfile, coverage_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent files for each radar can be extracted using the same function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, most_recent_file = s3enram.count_enram_coverage(level='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bejab': datetime.datetime(2016, 10, 9, 23, 50),\n",
       " 'bewid': datetime.datetime(2018, 1, 13, 17, 0),\n",
       " 'bezav': datetime.datetime(2016, 10, 9, 23, 50),\n",
       " 'bgvar': datetime.datetime(2016, 10, 9, 23, 55),\n",
       " 'ctcdv': datetime.datetime(2016, 10, 9, 23, 56),\n",
       " 'ctpda': datetime.datetime(2016, 10, 9, 23, 56),\n",
       " 'czbrd': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'czska': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'deboo': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dedrs': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deeis': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deemd': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deess': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'defbg': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'defld': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deflg': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dehnr': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deisn': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'demem': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deneu': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'denhb': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deoft': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'depro': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deros': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'desna': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'detur': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'deumd': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dkbor': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dkrom': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dksin': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dkste': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'dkvir': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'eehar': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'eesur': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'esalm': datetime.datetime(2018, 1, 15, 5, 0),\n",
       " 'esbad': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'esbar': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'escor': datetime.datetime(2017, 12, 30, 20, 0),\n",
       " 'eslid': datetime.datetime(2018, 1, 15, 5, 30),\n",
       " 'esmad': datetime.datetime(2018, 1, 15, 0, 30),\n",
       " 'esmal': datetime.datetime(2018, 1, 15, 4, 30),\n",
       " 'esmur': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'espma': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'essan': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'essev': datetime.datetime(2018, 1, 15, 5, 30),\n",
       " 'essse': datetime.datetime(2018, 1, 15, 4, 0),\n",
       " 'esval': datetime.datetime(2018, 1, 15, 7, 0),\n",
       " 'eszar': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fianj': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fiika': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fikes': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fikor': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fikuo': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'filuo': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fipet': datetime.datetime(2017, 11, 1, 7, 45),\n",
       " 'fiuta': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fivan': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'fivim': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frabb': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frale': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frave': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'frbla': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frbol': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frbor': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frbou': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frcae': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frche': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frcol': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frgre': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frlep': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frmcl': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frmom': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frmtc': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frnan': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frnim': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frniz': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'fropo': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frpla': datetime.datetime(2018, 1, 12, 11, 30),\n",
       " 'frtou': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frtra': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frtre': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'frtro': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'hrbil': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'hrosi': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'nldbl': datetime.datetime(2017, 1, 17, 13, 30),\n",
       " 'nldhl': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'nlhrw': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'plbrz': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plgda': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plleg': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plpas': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plpoz': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plram': datetime.datetime(2017, 11, 24, 14, 15),\n",
       " 'plrze': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'plswi': datetime.datetime(2017, 11, 24, 14, 0),\n",
       " 'ptfar': datetime.datetime(2016, 11, 30, 23, 56),\n",
       " 'ptliz': datetime.datetime(2016, 11, 30, 23, 56),\n",
       " 'ptprt': datetime.datetime(2016, 11, 30, 23, 56),\n",
       " 'seang': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'searl': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sease': datetime.datetime(2017, 5, 13, 9, 45),\n",
       " 'sehem': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sehud': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'sehuv': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sekir': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sekkr': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'selek': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'selul': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'seoer': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'seosd': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'seosu': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'seovi': datetime.datetime(2017, 8, 6, 4, 30),\n",
       " 'sevar': datetime.datetime(2016, 10, 9, 23, 45),\n",
       " 'sevax': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sevil': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'silis': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'sipas': datetime.datetime(2018, 1, 10, 17, 30),\n",
       " 'skjav': datetime.datetime(2018, 1, 15, 7, 30),\n",
       " 'skkoj': datetime.datetime(2018, 1, 15, 7, 30)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and saved to a file as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"radars_latest.csv\", 'w') as outfile:\n",
    "    dm.most_recent_to_csv(outfile, most_recent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countryradar,datetime\r",
      "\r\n",
      "bejab,2016-10-09 23:50\r",
      "\r\n",
      "bewid,2018-01-13 17:00\r",
      "\r\n",
      "bezav,2016-10-09 23:50\r",
      "\r\n",
      "bgvar,2016-10-09 23:55\r",
      "\r\n",
      "ctcdv,2016-10-09 23:56\r",
      "\r\n",
      "ctpda,2016-10-09 23:56\r",
      "\r\n",
      "czbrd,2016-10-09 23:45\r",
      "\r\n",
      "czska,2016-10-09 23:45\r",
      "\r\n",
      "deboo,2018-01-15 07:30\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head radars_latest.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Zip file support\n",
    "\n",
    "As downloading the individual `.h5` files from the website would be cumbersome, two options are available for easier data access:\n",
    "* Inclusion of a download function in the [BioRad](https://github.com/adokter/bioRad/blob/master/R/download_vp.R) R package, dedicated for bird profile research\n",
    "* Download of aggregated monthly dataset, provided as a zip-folders\n",
    "\n",
    "The preparation and creation of these zip-folders is supported by the `S3EnramHandler` module, using the `create_zip_version` function. The function uses a Counter with the key/counts or a list of keys from which the monthly counts will be derived as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As downloading the individual `.h5` files from the website would be cumbersome, two options are available for easier data access:\n",
    "* Inclusion of a download function in the [BioRad](https://github.com/adokter/bioRad/blob/master/R/download_vp.R) R package, dedicated for bird profile research\n",
    "* Download of aggregated monthly dataset, provided as a zip-folders\n",
    "\n",
    "The preparation and creation of these zip-folders is supported by the `S3EnramHandler` module, using the `create_zip_version` function. The function uses a Counter with the key/counts or a list of keys from which the monthly counts will be derived as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list of keys, the relevant month/radar combinations are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "keyset = ['bewid_vp_20161120233000.h5', \n",
    "          'bewid_vp_20161120233500.h5',\n",
    "          'bewid_vp_20161120234000.h5',\n",
    "          'bewid_vp_20161120234500.h5',\n",
    "          'bewid_vp_20161120235000.h5',\n",
    "          'bewid_vp_20161120235500.h5',\n",
    "          'bejab_vp_20161120235000.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s3enram.create_zip_version(keyset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical use-case is the update of those files that were transferred during a transfer operation, e.g. `btos.transferred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s3enram.create_zip_version(btos.transferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options are possible, e.g. update those zip files for a specific radar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "country = \"be\"\n",
    "radar = \"wid\"\n",
    "\n",
    "keyset = []\n",
    "for key in s3enram.list_files(path=\"/\".join([country, radar])):\n",
    "    keyset.append(os.path.split(key)[1])\n",
    "s3enram.create_zip_version(keyset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entire update can be done, by using the available coverage on a monthly or daily level (in comments, as this is a large operation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# s3client.create_zip_version(s3client.count_enram_coverage(level=\"month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential element in the file and folder handling, is that the (sub)folder information is inherent to the file name itself:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the file name for metadata, e.g. `dkrom_vp_20170114231500.h5`:\n",
    "\n",
    "* **country**: 2 characters `dk`\n",
    "* **radar**: 3 characters `rom`\n",
    "* ignore `_vp_`\n",
    "* **year**: 4 characters `2017`\n",
    "* **month**: 2 characters `01`\n",
    "* **day**: 2 characters `14`\n",
    "* **hour**: 2 characters `23`\n",
    "* **minutes**: 2 characters `00`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name parsing is provided by the `parse_filename` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dm.parse_filename(\"dkrom_vp_20170114231500.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
